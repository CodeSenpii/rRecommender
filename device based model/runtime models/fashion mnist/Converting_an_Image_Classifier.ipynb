{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Converting an Image Classifier.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzqpnNyD7IkU",
        "colab_type": "text"
      },
      "source": [
        "#Fashion Mnist tf.converter()\n",
        "\n",
        "The goal of this notebook is very clear\n",
        "\n",
        "\n",
        "*   training a model using convlution neural net\n",
        "*   convert the trained model to use in device based system\n",
        "\n",
        "If you are not looking for any of the above feel free to quit.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_wBIqbf7slk",
        "colab_type": "text"
      },
      "source": [
        "This notebook uses the **Fashion MNIST** dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXOyvpiT72ux",
        "colab_type": "text"
      },
      "source": [
        "![mnist](https://tensorflow.org/images/fashion-mnist-sprite.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w4l4XwE8AdU",
        "colab_type": "text"
      },
      "source": [
        "#SetUp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN8TkMpQ7IIL",
        "colab_type": "code",
        "outputId": "1d6d7148-ca46-43be-b676-9bdd7257a645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# TensorFlow\n",
        "import tensorflow as tf\n",
        "\n",
        "# TensorFlow Datsets\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# Helper Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "\n",
        "from os import getcwd\n",
        "\n",
        "print('\\u2022 Using TensorFlow Version:', tf.__version__)\n",
        "print('\\u2022 GPU Device Found.' if tf.test.is_gpu_available() else '\\u2022 GPU Device Not Found. Running on CPU')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "• Using TensorFlow Version: 2.2.0\n",
            "WARNING:tensorflow:From <ipython-input-1-d6bb89964d10>:16: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "• GPU Device Not Found. Running on CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NvIDXlB9PUF",
        "colab_type": "text"
      },
      "source": [
        "#Download Fashion Mnist Dataset\n",
        "We will use tensorflow dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJGY3dfn2iVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splits = tfds.Split.ALL.subsplit(weighted=(80,10,10))   #define train , validation, test portion\n",
        "\n",
        "filePath = f\"/tmp2/\"\n",
        "splits, info = tfds.load('fashion_mnist', with_info=True, as_supervised=True, split=['train[:80%]','train[80%:90%]','train[90%:100%]'], data_dir=filePath)\n",
        "(train_examples, validation_examples, test_examples) = splits\n",
        "\n",
        "num_examples = info.splits['train'].num_examples\n",
        "num_classes = info.features['label'].num_classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xzthhc-9ziv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt_top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1XwTehS-1fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a labels.txt file with the class names\n",
        "with open('labels.txt', 'w') as f:\n",
        "    f.write('\\n'.join(class_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSx_1xrx_9nE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The images in the dataset are 28 by 28 pixels.\n",
        "IMG_SIZE = 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJiJK9qYACvC",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi8C7WFdAGuw",
        "colab_type": "text"
      },
      "source": [
        "#Normalized Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_XM9Dsj__gK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def format_example(image, label):\n",
        "    # Cast image to float32\n",
        "    image = tf.image.convert_image_dtype(\n",
        "    image, dtype=\"float32\", saturate=False, name=None)\n",
        "        \n",
        "    # Normalize the image in the range [0, 1]\n",
        "    image = tf.image.per_image_standardization(image)\n",
        "    \n",
        "    return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f1c_-x_AJri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify the batch size\n",
        "BATCH_SIZE = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TziffaTiASkQ",
        "colab_type": "text"
      },
      "source": [
        "#Create Dataset from Labels And Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYg6Kre9AO8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Datasets\n",
        "train_batches = train_examples.cache().shuffle(num_examples//4).batch(BATCH_SIZE).map(format_example).prefetch(1)\n",
        "validation_batches = validation_examples.cache().batch(BATCH_SIZE).map(format_example)\n",
        "test_batches = test_examples.map(format_example).batch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDkioF4bAjvG",
        "colab_type": "text"
      },
      "source": [
        "#Building the Model¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1CuXXpNAoMv",
        "colab_type": "text"
      },
      "source": [
        "Model: \"sequential\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                   Output Shape              Param #   \n",
        "----------------------------------------------------------------\n",
        "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
        "_________________________________________________________________\n",
        "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
        "_________________________________________________________________\n",
        "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
        "_________________________________________________________________\n",
        "flatten (Flatten)            (None, 3872)              0         \n",
        "_________________________________________________________________\n",
        "dense (Dense)                (None, 64)                247872    \n",
        "_________________________________________________________________\n",
        "dense_1 (Dense)              (None, 10)                650       \n",
        "---------------------------------------------------------------------\n",
        "Total params: 253,322\n",
        "Trainable params: 253,322\n",
        "Non-trainable params: 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "873WaN22AfBk",
        "colab_type": "code",
        "outputId": "1a268213-bb6d-4666-9f45-017adaaeda25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    # Set the input shape to (28, 28, 1), kernel size=3, filters=16 and use ReLU activation,\n",
        "    tf.keras.layers.Conv2D(16,(3,3),activation=\"relu\",input_shape=(28,28,1)),\n",
        "      \n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "      \n",
        "    # Set the number of filters to 32, kernel size to 3 and use ReLU activation \n",
        "    tf.keras.layers.Conv2D(32,(3,3),activation=\"relu\"),\n",
        "      \n",
        "    # Flatten the output layer to 1 dimension\n",
        "    tf.keras.layers.Flatten(),\n",
        "      \n",
        "    # Add a fully connected layer with 64 hidden units and ReLU activation\n",
        "    tf.keras.layers.Dense(64,activation=\"relu\"),\n",
        "      \n",
        "    # Attach a final softmax classification head\n",
        "\n",
        "    tf.keras.layers.Dense(10, activation='softmax')])\n",
        "\n",
        "# Set the appropriate loss function and use accuracy as your metric\n",
        "model.compile(optimizer='adam',\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3872)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                247872    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 253,322\n",
            "Trainable params: 253,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KzW1TgRBF5f",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMlADr9BA-5a",
        "colab_type": "code",
        "outputId": "95e6e525-7ef0-4994-c248-13f60e9e34dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "history = model.fit(train_batches, epochs=10, validation_data=validation_batches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "188/188 [==============================] - 22s 120ms/step - loss: 0.5180 - accuracy: 0.8209 - val_loss: 0.3540 - val_accuracy: 0.8707\n",
            "Epoch 2/10\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.3362 - accuracy: 0.8804 - val_loss: 0.3057 - val_accuracy: 0.8835\n",
            "Epoch 3/10\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.2930 - accuracy: 0.8962 - val_loss: 0.2921 - val_accuracy: 0.8915\n",
            "Epoch 4/10\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.2631 - accuracy: 0.9056 - val_loss: 0.2715 - val_accuracy: 0.9007\n",
            "Epoch 5/10\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.2364 - accuracy: 0.9156 - val_loss: 0.2635 - val_accuracy: 0.9045\n",
            "Epoch 6/10\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.2184 - accuracy: 0.9224 - val_loss: 0.2528 - val_accuracy: 0.9078\n",
            "Epoch 7/10\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.1964 - accuracy: 0.9288 - val_loss: 0.2483 - val_accuracy: 0.9050\n",
            "Epoch 8/10\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.1838 - accuracy: 0.9336 - val_loss: 0.2398 - val_accuracy: 0.9108\n",
            "Epoch 9/10\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.1654 - accuracy: 0.9409 - val_loss: 0.2376 - val_accuracy: 0.9130\n",
            "Epoch 10/10\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.1497 - accuracy: 0.9463 - val_loss: 0.2351 - val_accuracy: 0.9140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHjD3L__BO55",
        "colab_type": "text"
      },
      "source": [
        "#Exporting to TFLite\n",
        "You will now save the model to TFLite. We should note, that you will probably see some warning messages when running the code below. These warnings have to do with software updates and should not cause any errors or prevent your code from running."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXV6zkR9BJtP",
        "colab_type": "code",
        "outputId": "9dd4db70-66d3-4e60-a4e6-0b35344e9604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# EXERCISE: Use the tf.saved_model API to save your model in the SavedModel format. \n",
        "export_dir = 'saved_model/1'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "tf.saved_model.save(model, export_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IhmrBznBX_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select mode of optimization\n",
        "mode = \"Speed\" \n",
        "\n",
        "if mode == 'Storage':\n",
        "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\n",
        "elif mode == 'Speed':\n",
        "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\n",
        "else:\n",
        "    optimization = tf.lite.Optimize.DEFAULT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXsCRTh-BdcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXERCISE: Use the TFLiteConverter SavedModel API to initialize the converter\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "\n",
        "# Set the optimzations\n",
        "converter.optimizations = [optimization]\n",
        "\n",
        "# Invoke the converter to finally generate the TFLite model\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYNAXt0dBm9S",
        "colab_type": "text"
      },
      "source": [
        "#Writing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uju1TkccBoRA",
        "colab_type": "code",
        "outputId": "3bd94d0d-aca1-4821-a5d3-e54913868f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tflite_model_file = pathlib.Path('./model.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "258672"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_2niATVBtRS",
        "colab_type": "text"
      },
      "source": [
        "#Test the model with tensorflow lite interpreter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1coxSVT4ByUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAdm7ijlB4Bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gather results for the randomly sampled test images\n",
        "predictions = []\n",
        "test_labels = []\n",
        "test_images = []\n",
        "\n",
        "for img, label in test_batches.take(50):\n",
        "    interpreter.set_tensor(input_index, img)\n",
        "    interpreter.invoke()\n",
        "    predictions.append(interpreter.get_tensor(output_index))\n",
        "    test_labels.append(label[0])\n",
        "    test_images.append(np.array(img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIIy4aabB8jc",
        "colab_type": "text"
      },
      "source": [
        "#Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpn6uJSaB96i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilities functions for plotting\n",
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    img = np.squeeze(img)\n",
        "    \n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "    \n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    \n",
        "    if predicted_label == true_label.numpy():\n",
        "        color = 'green'\n",
        "    else:\n",
        "        color = 'red'\n",
        "        \n",
        "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                         100*np.max(predictions_array),\n",
        "                                         class_names[true_label]),\n",
        "                                         color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "    predictions_array, true_label = predictions_array[i], true_label[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks(list(range(10)))\n",
        "    plt.yticks([])\n",
        "    thisplot = plt.bar(range(10), predictions_array[0], color=\"#777777\")\n",
        "    plt.ylim([0, 1])\n",
        "    predicted_label = np.argmax(predictions_array[0])\n",
        "    \n",
        "    thisplot[predicted_label].set_color('red')\n",
        "    thisplot[true_label].set_color('blue')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j2NwgTMCCqL",
        "colab_type": "code",
        "outputId": "946326b2-e088-4c79-a5e1-a27642202a12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# Visualize the outputs\n",
        "\n",
        "# Select index of image to display. Minimum index value is 1 and max index value is 50. \n",
        "index = 49 \n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(index, predictions, test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(index, predictions, test_labels)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGyIzUm5CNXR",
        "colab_type": "text"
      },
      "source": [
        "#Optimize For Storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dqsHuTyCPn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select mode of optimization\n",
        "mode = \"Storage\" \n",
        "\n",
        "if mode == 'Storage':\n",
        "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\n",
        "elif mode == 'Speed':\n",
        "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\n",
        "else:\n",
        "    optimization = tf.lite.Optimize.DEFAULT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3vWHXkVCcK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXERCISE: Use the TFLiteConverter SavedModel API to initialize the converter\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "\n",
        "# Set the optimzations\n",
        "converter.optimizations = [optimization]\n",
        "\n",
        "# Invoke the converter to finally generate the TFLite model\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUzmoOWyCg1J",
        "colab_type": "code",
        "outputId": "858b38aa-24e2-4b80-a9a0-c97086af980b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tflite_model_file = pathlib.Path('./model.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "258672"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssVlgG_KCqla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlQMSeKXC4LW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gather results for the randomly sampled test images\n",
        "predictions = []\n",
        "test_labels = []\n",
        "test_images = []\n",
        "\n",
        "for img, label in test_batches.take(50):\n",
        "    interpreter.set_tensor(input_index, img)\n",
        "    interpreter.invoke()\n",
        "    predictions.append(interpreter.get_tensor(output_index))\n",
        "    test_labels.append(label[0])\n",
        "    test_images.append(np.array(img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTaln4a7C80S",
        "colab_type": "code",
        "outputId": "ab190dbb-c4ee-443e-d451-d3867450ed42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# Visualize the outputs\n",
        "\n",
        "# Select index of image to display. Minimum index value is 1 and max index value is 50. \n",
        "index = 49 \n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(index, predictions, test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(index, predictions, test_labels)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}