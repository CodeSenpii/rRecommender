{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p1_tokenization_of_sentence.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI6Y3-Qay3ko",
        "colab_type": "text"
      },
      "source": [
        "#Hi Welcome to The NoteBook \n",
        "##We will implement tokenization API in keras to encode sentences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtFMhQdwzObL",
        "colab_type": "text"
      },
      "source": [
        "#Import Relevent library\n",
        "We as part of tokenization required <b>Tokenizer </b> from keras preprocess API and pad_sequence for padding in this notecook we will implement different operations possible with this functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqPSWWSsylGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence  import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AulICyU80Hjp",
        "colab_type": "text"
      },
      "source": [
        "#Define dummy Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHKyxDj8z_NV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!',\n",
        "    'Do you think my dog is amazing?'\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJmz45GU0S7R",
        "colab_type": "text"
      },
      "source": [
        "initiate tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pbzZPl70Qvi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "53c82e2a-cd47-4305-a753-c221e1b28cc6"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100,oov_token=\"UNK\")\n",
        "#fit on text\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "#see the word index\n",
        "tokenizer.word_index"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'UNK': 1,\n",
              " 'amazing': 11,\n",
              " 'cat': 7,\n",
              " 'do': 8,\n",
              " 'dog': 4,\n",
              " 'i': 5,\n",
              " 'is': 10,\n",
              " 'love': 3,\n",
              " 'my': 2,\n",
              " 'think': 9,\n",
              " 'you': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwSpLTSE021G",
        "colab_type": "text"
      },
      "source": [
        "##Create Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbXhBZAi0s9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a2495b12-c0e6-40ea-9342-1a787df11293"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "padded = pad_sequences(sequences=sequences,maxlen=10)\n",
        "print(sequences)\n",
        "print(padded)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "[[ 0  0  0  0  0  0  5  3  2  4]\n",
            " [ 0  0  0  0  0  0  5  3  2  7]\n",
            " [ 0  0  0  0  0  0  6  3  2  4]\n",
            " [ 0  0  0  8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt-4zS3L1Y34",
        "colab_type": "text"
      },
      "source": [
        "As we can see the padding is done on pre side and no truncation happens due to every sentences is less than 10 words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdGwpxtr1VPC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63bf1b81-c052-455f-eaae-fe33cca80336"
      },
      "source": [
        "test_data = [\"I am learning from coursera\",\n",
        "             \"It is very sad to see people laugh at others for not liking their dog's\"]\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 1, 1, 1, 1], [1, 10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VmJBciH2FfT",
        "colab_type": "text"
      },
      "source": [
        "most of the words are not from my dictonary so replaced by unk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc6c4ofW2DRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "81b71f71-fb38-4428-e4aa-e6f6b8eb1973"
      },
      "source": [
        "padded_test = pad_sequences(test_seq,maxlen=10)\n",
        "print(padded_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 5 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak557nMI-s0x",
        "colab_type": "text"
      },
      "source": [
        "#Written by : SATYAJIT MAITRA\n",
        "## follow github: https://github.com/MachineLearningWithHuman/\n",
        "## Email for work : ssatyajitmaitra@gmail.com\n",
        "##learn with me on youtube: https://www.youtube.com/channel/UCiWd572-4LeH0IqJ5A7LavA/\n",
        "### please give a star if it helps you "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwc95Lb8-smm",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}