{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "write like Shakespeare.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTrDvtWj5jv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHITDyRX5zEp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4a38dcb3-d391-4a12-912a-b753e5fe2f24"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n",
        "    -O /tmp/sonnets.txt\n",
        "data = open('/tmp/sonnets.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "label = ku.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-11 08:06:06--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.121.128, 74.125.202.128, 209.85.234.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.121.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93578 (91K) [text/plain]\n",
            "Saving to: ‘/tmp/sonnets.txt’\n",
            "\n",
            "\r/tmp/sonnets.txt      0%[                    ]       0  --.-KB/s               \r/tmp/sonnets.txt    100%[===================>]  91.38K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-07-11 08:06:07 (88.7 MB/s) - ‘/tmp/sonnets.txt’ saved [93578/93578]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btQua9J255xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "1d987d74-752f-482f-9732-2e03e37ecb6c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 10, 100)           321100    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 10, 300)           301200    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 10, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1605)              162105    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3211)              5156866   \n",
            "=================================================================\n",
            "Total params: 6,101,671\n",
            "Trainable params: 6,101,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP4bRpvu58lF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77ac7497-d6b2-4f97-ebe7-91811137be10"
      },
      "source": [
        " history = model.fit(predictors, label, epochs=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 6.9082 - accuracy: 0.0226\n",
            "Epoch 2/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 6.4968 - accuracy: 0.0224\n",
            "Epoch 3/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 6.3845 - accuracy: 0.0251\n",
            "Epoch 4/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 6.2617 - accuracy: 0.0312\n",
            "Epoch 5/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 6.1727 - accuracy: 0.0362\n",
            "Epoch 6/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 6.0870 - accuracy: 0.0384\n",
            "Epoch 7/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 6.0102 - accuracy: 0.0415\n",
            "Epoch 8/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 5.9203 - accuracy: 0.0427\n",
            "Epoch 9/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 5.8127 - accuracy: 0.0525\n",
            "Epoch 10/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 5.7047 - accuracy: 0.0592\n",
            "Epoch 11/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 5.5967 - accuracy: 0.0614\n",
            "Epoch 12/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 5.4844 - accuracy: 0.0692\n",
            "Epoch 13/100\n",
            "484/484 [==============================] - 36s 74ms/step - loss: 5.3795 - accuracy: 0.0744\n",
            "Epoch 14/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 5.2749 - accuracy: 0.0832\n",
            "Epoch 15/100\n",
            "484/484 [==============================] - 36s 74ms/step - loss: 5.1619 - accuracy: 0.0874\n",
            "Epoch 16/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 5.0518 - accuracy: 0.0976\n",
            "Epoch 17/100\n",
            "484/484 [==============================] - 36s 74ms/step - loss: 4.9451 - accuracy: 0.1044\n",
            "Epoch 18/100\n",
            "484/484 [==============================] - 39s 81ms/step - loss: 4.8327 - accuracy: 0.1135\n",
            "Epoch 19/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 4.7259 - accuracy: 0.1240\n",
            "Epoch 20/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 4.6205 - accuracy: 0.1340\n",
            "Epoch 21/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 4.5172 - accuracy: 0.1414\n",
            "Epoch 22/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 4.4012 - accuracy: 0.1503\n",
            "Epoch 23/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 4.2999 - accuracy: 0.1662\n",
            "Epoch 24/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 4.1865 - accuracy: 0.1766\n",
            "Epoch 25/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 4.0844 - accuracy: 0.1903\n",
            "Epoch 26/100\n",
            "484/484 [==============================] - 36s 75ms/step - loss: 3.9799 - accuracy: 0.2085\n",
            "Epoch 27/100\n",
            "484/484 [==============================] - 38s 80ms/step - loss: 3.8817 - accuracy: 0.2227\n",
            "Epoch 28/100\n",
            "484/484 [==============================] - 37s 76ms/step - loss: 3.7894 - accuracy: 0.2401\n",
            "Epoch 29/100\n",
            "484/484 [==============================] - 37s 76ms/step - loss: 3.6829 - accuracy: 0.2588\n",
            "Epoch 30/100\n",
            "484/484 [==============================] - 37s 76ms/step - loss: 3.5821 - accuracy: 0.2787\n",
            "Epoch 31/100\n",
            "484/484 [==============================] - 37s 76ms/step - loss: 3.4852 - accuracy: 0.2998\n",
            "Epoch 32/100\n",
            "484/484 [==============================] - 37s 76ms/step - loss: 3.3995 - accuracy: 0.3199\n",
            "Epoch 33/100\n",
            "484/484 [==============================] - 37s 76ms/step - loss: 3.3129 - accuracy: 0.3420\n",
            "Epoch 34/100\n",
            "484/484 [==============================] - 39s 81ms/step - loss: 3.2364 - accuracy: 0.3576\n",
            "Epoch 35/100\n",
            "484/484 [==============================] - 37s 77ms/step - loss: 3.1508 - accuracy: 0.3804\n",
            "Epoch 36/100\n",
            "484/484 [==============================] - 37s 77ms/step - loss: 3.0683 - accuracy: 0.3987\n",
            "Epoch 37/100\n",
            "484/484 [==============================] - 37s 77ms/step - loss: 2.9945 - accuracy: 0.4129\n",
            "Epoch 38/100\n",
            "484/484 [==============================] - 37s 77ms/step - loss: 2.9253 - accuracy: 0.4257\n",
            "Epoch 39/100\n",
            "484/484 [==============================] - 37s 76ms/step - loss: 2.8494 - accuracy: 0.4455\n",
            "Epoch 40/100\n",
            "484/484 [==============================] - 37s 77ms/step - loss: 2.7767 - accuracy: 0.4636\n",
            "Epoch 41/100\n",
            "484/484 [==============================] - 37s 77ms/step - loss: 2.7111 - accuracy: 0.4764\n",
            "Epoch 42/100\n",
            "484/484 [==============================] - 37s 77ms/step - loss: 2.6475 - accuracy: 0.4948\n",
            "Epoch 43/100\n",
            "484/484 [==============================] - 37s 77ms/step - loss: 2.5868 - accuracy: 0.5083\n",
            "Epoch 44/100\n",
            "484/484 [==============================] - 37s 77ms/step - loss: 2.5220 - accuracy: 0.5210\n",
            "Epoch 45/100\n",
            "484/484 [==============================] - 37s 76ms/step - loss: 2.4742 - accuracy: 0.5318\n",
            "Epoch 46/100\n",
            "484/484 [==============================] - 37s 76ms/step - loss: 2.4082 - accuracy: 0.5487\n",
            "Epoch 47/100\n",
            "142/484 [=======>......................] - ETA: 26s - loss: 2.2323 - accuracy: 0.5836"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZi7H3UQ5-mR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLtdOXGI6Au0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh0d1Yy35At6",
        "colab_type": "text"
      },
      "source": [
        "* Written by : SATYAJIT MAITRA\n",
        "*  follow github: https://github.com/MachineLearningWithHuman/\n",
        "*  Email for work : ssatyajitmaitra@gmail.com\n",
        "* learn with me on youtube: https://www.youtube.com/channel/UCiWd572-4LeH0IqJ5A7LavA/\n",
        "* please give a star if it helps you"
      ]
    }
  ]
}